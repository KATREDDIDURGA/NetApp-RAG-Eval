id,question,answer,citations,confidence,keyword_hit_rate,must_include_hit,has_citations,unique_sources,length_chars,length_score,blended_score,retrieval_mode,latency_sec,contexts_json
1,What are the key benefits of Generative AI in NetApp's value proposition?,"NetApp's value proposition for Generative AI includes intelligent data management software and storage infrastructure optimized for AI workloads, data mobility, data governance, and data security technologies across the edge-to-core-to-cloud ecosystem. NetApp's products and solutions, such as NetApp ONTAP AI, NetApp SnapMirror, and NetApp FlexCache, help customers build a free-flowing data pipeline for data preparation, data protection, and strategic data management responsibilities of AI model training and inferencing. These capabilities enable seamless data management and data movement across the AI data pipeline for training, retraining, fine-tuning, and inferencing generative AI models, optimizing the performance and scalability of the AI/ML lifecycle.","['[Generative_AI_and_NetApp_Value.html:1]', '[Generative_AI_and_NetApp_Value.html:2]', '[Generative_AI_and_NetApp_Value.html:3]', '[Generative_AI_and_NetApp_Value.html:4]', '[Generative_AI_and_NetApp_Value.html:5]']",0.9,0.0,1.0,True,1,766,1.0,0.555,vector,15.614,"[{""content"": ""As a result, phrases like Stable Diffusion, Hallucinations, Prompt Engineering and Value Alignment are rapidly emerging in the design of AI systems. These self-supervised or semi-supervised machine learning (ML) models are becoming widely available as pre-trained foundation models (FM) via cloud service providers and other AI firmsvendors, which are being adopted by various business establishments across industries for a wide range of downstream NLP (natural language processing) tasks. As asserted by research analyst firms like McKinsey – “Generative AI’s impact on productivity could add trillions of dollars in value to the global economy.” While companies are reimagining AI as thought partners to humans and FMs are broadening simultaneously to what businesses and institutions can do with generative AI, the opportunities to manage massive volumes of data will continue to grow. This document presents introductory information on generative AI and the design concepts in relation to NetApp capabilities that bring value to NetApp customers, both on-premises and hybrid or multicloud environments. So, what’s in it for customers to use NetApp in their AI environments? NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI. NetApp has combined various capabilities into intelligent data management software and storage infrastructure that have been well balanced with high-performance optimized for AI workloads."", ""metadata"": {""source_type"": ""html"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 1}, ""score"": 0.2203235626220703}, {""content"": ""Generative AI and NetApp Value The demand for generative artificial intelligence (AI) is driving disruption across industries, enhancing business creativity and product innovation. Abstract Many organizations are using generative AI to build new product features, improve engineering productivity and prototype AI powered applications that deliver better results and consumer experiences. Generative AI such as Generative Pre-trained Transformers (GPT) use neural networks to create new content, as diverse as text, audio, and video. Given the extreme scale and massive datasets involved with large language models (LLMs), it is crucial to architect a robust AI infrastructure that takes advantage of the compelling data storage features of on-premises, hybrid and multicloud deployment options and reduce risks associated with data mobility, data protection and governance before companies can design AI solutions. This paper describes these considerations and the corresponding NetApp® AI capabilities that enable seamless data management and data movement across the AI data pipeline for training, retraining, fine-tuning, and inferencing generative AI models. Executive Summary Most recently after the launch of ChatGPT, a spin-off of GPT-3 in November 2022, new AI tools used to generate text, code, image, or even therapeutic proteins in response to user prompts have gained significant fame. This indicates users can make a request using natural language and AI will interpret and generate text, such as news articles or product descriptions that reflect user request or produce code, music, speech, visual effects, and 3D assets using algorithms trained on already existing data."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 0, ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""source_type"": ""html""}, ""score"": 0.2152111530303955}, {""content"": ""Generative AI solutions like LLMs need to read and process their source datasets from storage into memory numerous times to foster intelligence. NetApp has been a leader in data mobility, data governance and data security technologies across the edge-to-core-to-cloud ecosystem, serving enterprise customers build at-scale AI solutions. NetApp, with a strong network of partners has been helping chief data officers, AI engineers, enterprise architects and data scientists in the design of a free-flowing data pipeline for data preparation, data protection, and strategic data management responsibilities of AI model training and inferencing, optimizing the performance and scalability of the AI/ML lifecycle. NetApp data technologies and capabilities such as NetApp® ONTAP AI® for deep learning data pipeline, NetApp® SnapMirror® for transporting data seamlessly and efficiently between storage endpoints, and NetApp® FlexCache® for real-time rendering when the data flow shifts from batch to real-time and data engineering happens at prompt time, bring value to the deployment of real-time Generative AI models. As enterprises of all types embrace new AI tools, they face data challenges from the edge to the data center to the cloud that demand for scalable, responsible and explainable AI solutions. As the data authority on hybrid and multi cloud, NetApp is committed to building a network of partners and joint solutions that can help with all aspects of constructing a data pipeline and data lakes for generative AI model training (pre-training), fine-tuning, context-based inferencing and model decay monitoring of LLMs."", ""metadata"": {""source_type"": ""html"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 2}, ""score"": 0.1828569769859314}, {""content"": ""Third, NetApp brings customer value to the AI data pipeline and ML/LLM workflow via products and solutions serving customers building predictive AI solutions such as demand forecasting, medical imaging, sentiment analysis, and generative AI solutions like GANs for industrial images anomaly detection in manufacturing sector and anti-money laundering and fraud detection in banking & financial services with NetApp products and capabilities like NetApp® ONTAP AI®, NetApp® SnapMirror® , and NetApp® FlexCache®. NetApp capabilities The movement and management of data in generative AI applications such as chatbot, code generation, image generation or genome model expression can span across the edge, private data center, and hybrid multicloud ecosystem. For instance, a real-time AI-bot helping a passenger upgrade his or her airline ticket to business class from an end-user app exposed via APIs of pre-trained models such as ChatGPT cannot achieve that task by itself since the passenger information is not publicly available on the internet. The API requires access to the passenger’s personal info and ticket info from the airline carrier which may exist in a hybrid or multicloud ecosystem. A similar scenario might apply to scientists sharing a drug molecule and patient data via an end-user application that uses LLMs to accomplish clinical trials across drug discovery involving one-to-many bio-medical research institutions."", ""metadata"": {""chunk_id"": 12, ""source_type"": ""html"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""doc_id"": ""6ae84725f8f4dd1a""}, ""score"": 0.17989081144332886}, {""content"": ""Inaccurate outcomes or biased results generated by the models can put both businesses and consumers in jeopardy. These risks can correspond to constraints for LLMs emerging potentially from data management challenges associated with data quality, data security, and data mobility. NetApp helps organizations meet the complexities created by rapid data growth, data mobility, multi-cloud management, and the adoption of AI. At scale AI infrastructure and efficient data management is crucial to defining the success of AI applications like generative AI. It is critical customers cover all the deployment scenarios without compromising on the ability to expand as enterprises need to while maintaining cost-efficiency, data governance and ethical AI practices in control. NetApp is constantly working to help customers simplify and accelerate their AI deployments."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""source_type"": ""html"", ""chunk_id"": 20}, ""score"": 0.15628701448440552}, {""content"": ""NVIDIA AI Enterprise, powered by NetApp and VMware, delivers enterprise-class AI workload and data management in a simplified, familiar package. 1P Cloud Platforms Fully managed cloud storage offerings are available natively on Microsoft Azure as Azure NetApp Files (ANF), on AWS as Amazon FSx for NetApp ONTAP (FSx ONTAP), and on Google as Google Cloud NetApp Volumes (GNCV). 1P is a managed, high-performance file system that enables customers to run highly available AI workloads with improved data security in public clouds, for fine-tuning LLMs/FMs with cloud native ML platforms like AWS SageMaker, Azure-OpenAI Services, and Google’s Vertex AI. NetApp Partner Solution Suite In addition to its core data products, technologies and capabilities, NetApp also collaborates closely with a robust network of AI partners to bring added value to customers. NVIDIA Guardrails in AI systems serve as safeguards to ensure the ethical and responsible use of AI technologies. AI developers can choose to define the behavior of LLM-powered applications on specific topics and prevent them from engaging in discussions on unwanted topics. Guardrails, an open-source toolkit, provides the ability to connect an LLM to other services, seamlessly and securely for building trustworthy, safe, and secure LLM conversational systems. Domino Data Lab provides versatile, enterprise-grade tools for building and productizing Generative AI - fast, safe, and economical, wherever you are in your AI journey."", ""metadata"": {""chunk_id"": 18, ""source_type"": ""html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""file_name"": ""Generative_AI_and_NetApp_Value.html""}, ""score"": 0.044069647789001465}, {""content"": ""ONTAP includes various data management and protection features and capabilities, including automatic ransomware protection against cyber-attacks, built-in data transport features, and storage efficiency capabilities for a range of architectures from on-premises, hybrid, multiclouds in NAS, SAN, object, and software defined storage (SDS) situations of LLM deployments. NetApp® ONTAP AI® for deep learning model training. NetApp® ONTAP® supports NVIDIA GPU Direct StorageTM with the use of NFS over RDMA for NetApp customers with ONTAP storage cluster and NVIDIA DGX compute nodes . It offers a cost-efficient performance to read and process source datasets from storage into memory numerous times to foster intelligence, enabling organizations with training, fine-tuning, and scaling access to LLMs. NetApp® FlexCache® is a remote caching capability that simplifies file distribution and caches only the actively read data. This can be useful for LLM training, re-training, and fine tuning, bringing value to customers with business requirements like real-time rendering and LLM inferencing. NetApp® SnapMirror is an ONTAP feature that replicates volume snapshots between any two ONTAP systems. This feature optimally transfers data at the edge to your on-premises data center or to the cloud. SnapMirror can be used for moving data securely and efficiently between on-premises and hyperscaler clouds, when customers want to develop generative AI in clouds with RAG containing enterprise data."", ""metadata"": {""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""source_type"": ""html"", ""chunk_id"": 14, ""doc_id"": ""6ae84725f8f4dd1a""}, ""score"": 0.0191270112991333}, {""content"": ""A free-flowing data pipeline with a robust AI infrastructure that supports data mobility, data quality, data governance and data protection via end-to-end encryption and AI guardrails is eminent in the design of responsible and explainable generative AI models. Customer scenario and NetApp Figure 3: Machine Learning/Large Language Model Workflow Are we training or fine-tuning? The question of whether to (a) train an LLM model from scratch, fine-tune a pre-trained FM, or use RAG to retrieve data from document repositories outside a foundation model and augment prompts, and (b) either by leveraging open-source LLMs (E.g., Llama 2) or proprietary FMs (E.g., ChatGPT, Bard, AWS Bedrock) is a strategic decision for organizations. Each approach has a tradeoff between cost-efficiency, data gravity, operations, model accuracy and management of LLMs. NetApp as a company embraces AI internally in its work culture and in its approach to product design and engineering efforts. For instance, NetApp's autonomous ransomware protection is built using AI and machine learning. It provides early detection of file system anomalies to help identify threats before they impact operations. Second, NetApp uses predictive AI for its business operations like sales and inventory forecasting and chatbots to assist customers in call center product support services, tech specs, warranty, service manuals, and more."", ""metadata"": {""chunk_id"": 11, ""source_type"": ""html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""file_name"": ""Generative_AI_and_NetApp_Value.html""}, ""score"": 0.017146587371826172}]"
2,How can I improve LLM accuracy and performance with PGVector and PostgreSQL?,"To improve LLM accuracy and performance with PGVector and PostgreSQL, you can enable the PGVector extension in your PostgreSQL database, set up a table for storing documents and their embeddings, scrape and extract information from a source, create embeddings for the documents, and store those embeddings in the database. This allows for precise and rapid searches through extensive datasets and enables users to perform natural language queries while saving time sifting through irrelevant data. The combined power of PostgreSQL and PGVector is a game-changing transformation for data retrieval.",['[How_To_Improve_Your_LLM_Accuracy_and_Performance_With_PGVector_and_PostgreSQL.html:1]'],0.9,0.0,1.0,True,1,597,1.0,0.555,hybrid,19.474,"[{""content"": ""This now makes it possible to sift through vast amounts of textual data in a seamless way. With PGVector, PostgreSQL can conduct exact nearest neighbor search and approximate nearest neighbor search using L2 (or Euclidian) distance, inner product, and cosine distance. OpenAI particularly recommends the cosine distance for its efficiency in capturing semantic similarities. The Role of Embeddings in Retrieval Augmented Generation (RAG) and LLMs One of the more fascinating applications of embeddings is their role in the Retrieval Augmented Generation (RAG) process. In this technique, relevant information is retrieved from an external source, transformed in a way that makes it digestible by an LLM, and subsequently fed into the LLM for generated text output. This allows the LLM to be fine-tuned in knowledge it wasn’t originally trained on. Take, for example, your company’s documentation as source: from here you can generate embeddings and store them in PostgreSQL. Now, when a user queries the documentation, the numerical representation of the user’s query can be computed, and the relevant pieces of the documentation are identified and retrieved from the database by performing a similarity search on the embeddings. The retrieved documentation gets passed together with the user’s query to the LLM, which provides us with an accurate answer as well as the relevant pieces of the documentation and its sources."", ""metadata"": {""doc_id"": ""5c2b2c2734bfb5c2"", ""chunk_id"": 1, ""file_name"": ""How_To_Improve_Your_LLM_Accuracy_and_Performance_With_PGVector_and_PostgreSQL.html"", ""source_type"": ""html""}, ""score"": 0.009523809523809523, ""rerank_score"": 5.105644226074219}, {""content"": ""To swiftly bring our concept to life, we utilized Streamlit, a powerful tool for building interactive Python interfaces. Wrapping It Up: Super-Efficient Data Retrieval On our tech expedition today, we ventured deep into the realms of embeddings, tapping into the combined power of PostgreSQL and PGVector. We stored embeddings alongside the original data, allowing us to perform precise and rapid searches even through extensive datasets. Finally, we streamlined data retrieval through LLMs, allowing users the ability to perform natural language queries while saving them the time of sifting through irrelevant data. The combined power of PostgreSQL and PGVector is a game-changing transformation for how we can now retrieve data. Storing embeddings alongside the original data allows us to perform precise and rapid searches (even through extensive datasets), and by streamlining data retrieval through LLMs we can give users the ability to perform natural language queries while saving them the time of sifting through irrelevant data. Here’s to a future where data retrieval is not just smart—but super-efficient! Written by Solutions Engineer CTO Ready to try out PGVector on your own dataset? Get your free trial of Instaclustr Platform today and spin up a cluster in no time!"", ""metadata"": {""chunk_id"": 6, ""doc_id"": ""5c2b2c2734bfb5c2"", ""file_name"": ""How_To_Improve_Your_LLM_Accuracy_and_Performance_With_PGVector_and_PostgreSQL.html"", ""source_type"": ""html""}, ""score"": 0.009836065573770491, ""rerank_score"": 3.449007034301758}, {""content"": ""Why Use PGVector? We’ve all been in this position before: trying to quickly solve a technical problem just to end up spending hours browsing through endless documentation in the hope of finding a solution. With all that time going to waste, there has to be a better way to search documentation. And now, there is. Instaclustr has recently introduced the PGVector extension to our PostgreSQL Managed Service, enabling users to quickly and easily search text data without the need for a specialized database to store your embeddings—a major development in the realm of textual data. Embeddings are a mathematical representation of how related or similar words are. For example, the words “tree” and “bush” would be more closely related than say, “tree” and “automobile.” Embeddings are generally stored as vectors, which are essentially a list of numbers. PGVector is an extension that provides the ability to search for vectors that are more closely related to each other. With embeddings and vector search, you can then search for text that semantically means the same thing—an incredibly useful tool, especially when building Large Language Models (LLMs). When PGVector comes into play, PostgreSQL turns into a highly performant vector store capable of performing distance-based search operations on embeddings."", ""metadata"": {""chunk_id"": 0, ""doc_id"": ""5c2b2c2734bfb5c2"", ""file_name"": ""How_To_Improve_Your_LLM_Accuracy_and_Performance_With_PGVector_and_PostgreSQL.html"", ""source_type"": ""html""}, ""score"": 0.00967741935483871, ""rerank_score"": 1.1642723083496094}, {""content"": ""Implementation Guidance and User Interaction First, let’s enable the PGVector extension in our PostgreSQL database and set up a table for storing all our documents along with their embeddings: | 1 2 3 4 5 | CREATE EXTENSION vector; CREATE TABLE insta_documentation (id bigserial PRIMARY KEY, title, content, url, embedding vector(3)); | Let’s now write some Python code to scrape the Instaclustr documentation page, extract the main text parts like title and content using Beautiful Soup and store them together with the URL in our PostgreSQL table: | 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 | urls = [...] def init_connection(): return psycopg2.connect(**st.secrets[\""postgres\""]) def extract_info(url): hdr = {'User-Agent': 'Mozilla/5.0'} req = Request(url,headers=hdr) response = urlopen(req) soup = BeautifulSoup(response, 'html.parser') title = soup.find('title').text middle_section = soup.find('div', class_='documentation-middle').contents # middle section consists of header, content and instaclustr banner and back and forth links - we want only the first two content = str(middle_section[0]) + str(middle_section[1]) return title, content, url conn = init_connection() cursor = conn.cursor() for url in urls: page_content = extract_info(url) postgres_insert_query = \""\""\"" INSERT INTO insta_documentation (title, content, url) VALUES (%s, %s, %s)\""\""\"" cursor.execute(postgres_insert_query, page_content) conn.commit() if conn: cursor.close() conn.close() | Next, it’s time to load the documentation pages from the database, split them up into chunks and create the pivotal embeddings, and then store those embeddings in the database: | 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 | def init_connection(): return psycopg2.connect(**st.secrets[\""postgres\""]) conn = init_connection() cursor = conn.cursor() # Define and execute query to the insta_documentation table, limiting to 10 results for testing (creating embeddings through the OpenAI API can get costly when dealing with a huge amount of data) postgres_query = \""\""\"" SELECT title, content, url FROM insta_documentation LIMIT 10\""\""\"" cursor.execute(postgres_query) results = cursor.fetchall() conn.commit() # Load results into pandas DataFrame for easier manipulation df = pd.DataFrame(results, columns=['title', 'content', 'url']) # Break down content text which exceed max input token limit into smaller chunk documents # Define text splitter html_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.HTML, chunk_size=1000, chunk_overlap=100) # We need to initialize our embeddings model embeddings = OpenAIEmbeddings(model=\""text-embedding-ada-002\"") docs = [] for i in range(len(df.index)): # Create document with metadata for each content chunk docs = docs + html_splitter.create_documents([df['content'][i]], metadatas=[{\""title\"": df['title'][i], \""url\"": df['url'][i]}]) # Create pgvector dataset db = PGVector.from_documents( embedding=embeddings, documents=docs, collection_name=COLLECTION_NAME, connection_string=CONNECTION_STRING, distance_strategy=DistanceStrategy.COSINE, ) | And finally, watch in awe as the retriever springs into action, pinpointing the right pieces every time a query is dropped: | 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 | query = st.text_input('Your question', placeholder='How can I sign up for an Instaclustr console account?') retriever = store.as_retriever(search_kwargs={\""k\"": 3}) qa = RetrievalQA.from_chain_type( llm=OpenAI(), chain_type=\""stuff\"", retriever=retriever, return_source_documents=True, verbose=True, ) result = qa({\""query\"": query}) source_documents = result[\""source_documents\""] document_page_content = [document.page_content for document in source_documents] document_metadata = [document.metadata for document in source_documents] | Now, let’s test the system and view a successful query in action."", ""metadata"": {""doc_id"": ""5c2b2c2734bfb5c2"", ""chunk_id"": 5, ""source_type"": ""html"", ""file_name"": ""How_To_Improve_Your_LLM_Accuracy_and_Performance_With_PGVector_and_PostgreSQL.html""}, ""score"": 0.00923076923076923, ""rerank_score"": -0.3743760883808136}, {""content"": ""Once the pre-trained model is selected, the next step is to fine-tune it on the domain-specific data. This involves adjusting the model’s parameters and training it on the new data to adapt to a specific domain and task. For example, BloombergGPT, a proprietary LLM trained on a wide range of financial data serving the financial industry. Domain-specific models designed and trained for a specific task generally have higher accuracy and performance within their scope, but low transferability across other tasks or domains. When business environment and data change over a period, the prediction accuracy of the FM could begin to decline when compared to their performance during testing. This is when retraining or fine-tuning the model becomes crucial. Model retraining in traditional AI/ML refers to updating a deployed ML model with new data, generally performed to eliminate two types of drifts that occur. (1) Concept drift – when the link between the input variables and the target variables changes over time, since the description of what we want to predict changes, the model can produce inaccurate predictions. (2) Data drift – occurs when the characteristics of the input data change, like changes in customer habits or behavior over time and therefore the model’s inability to respond to such changes."", ""metadata"": {""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""source_type"": ""html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 8}, ""score"": 0.008333333333333333, ""rerank_score"": -2.4084010124206543}, {""content"": ""Let’s imagine that someone is searching Instaclustr documentation with a couple of phrases: - “Configure hard drive failure setting in Apache Cassandra” - “Change storage settings in Redis” - “Enterprise pricing for a 2-year commitment” - “Raise a support ticket” - “Connect to PostgreSQL using websockets” When you plot the embeddings of the above phrases, the first 2 are going to be closer than the rest due to the semantic meaning of each sentence (changing how these databases work with storage), despite not sharing any of the same keywords: However, one challenge remains: the ‘context window’ of every LLM. This window refers to the number of tokens an LLM can process at once. Models with a limited context window tend to falter with larger inputs. On the flip side, models trained with large context windows (e.g., 100,000 tokens—enough to provide an entire book as part of your prompt) will see an increase in latency of a response and require storing the entire context, weights, k/v store, etc., in memory of the GPU running the model. This means we have an incentive to keep the context window as small as possible to generate useful answers. With embeddings, we can perform simple distance searches on our text and use this to only pull the relevant pieces of data to provide to the LLM."", ""metadata"": {""chunk_id"": 2, ""file_name"": ""How_To_Improve_Your_LLM_Accuracy_and_Performance_With_PGVector_and_PostgreSQL.html"", ""source_type"": ""html"", ""doc_id"": ""5c2b2c2734bfb5c2""}, ""score"": 0.008823529411764706, ""rerank_score"": -2.4118154048919678}, {""content"": ""However, the workflow and tech stack for LLMs could vary in some ways. For instance, LLM pipelines built using frameworks like LangChain string together multiple LLM API calls to external embedding endpoints such as vectorstores or vector databases. The use of an embedding endpoint and vectorstore for downstream connectors (like to a vector database) represents a significant development in how data is stored and accessed. As opposed to traditional ML models that are developed from scratch, LLMs often rely on transfer learning since these models start with FMs that are fine-tuned with new data to improve performance in a more specific domain. Therefore, it is crucial LLMOps deliver the capabilities of risk management and model decay monitoring. Risks and Ethics in the age of Generative AI “ChatGPT – It’s slick but still spews nonsense.”– MIT Tech Review. Garbage in–garbage out, has always been the challenging case with computing. The only difference with generative AI is that it excels at making the garbage highly credible, leading to inaccurate outcomes. LLMs are prone to invent facts to fit the narrative it’s building. Therefore, companies that see generative AI as a great opportunity to lower their costs with AI equivalents need to efficiently detect deep fakes, reduce biases, and lower risks to keep the systems honest and ethical."", ""metadata"": {""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""source_type"": ""html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 10}, ""score"": 0.00909090909090909, ""rerank_score"": -2.490992546081543}, {""content"": ""ONTAP includes various data management and protection features and capabilities, including automatic ransomware protection against cyber-attacks, built-in data transport features, and storage efficiency capabilities for a range of architectures from on-premises, hybrid, multiclouds in NAS, SAN, object, and software defined storage (SDS) situations of LLM deployments. NetApp® ONTAP AI® for deep learning model training. NetApp® ONTAP® supports NVIDIA GPU Direct StorageTM with the use of NFS over RDMA for NetApp customers with ONTAP storage cluster and NVIDIA DGX compute nodes . It offers a cost-efficient performance to read and process source datasets from storage into memory numerous times to foster intelligence, enabling organizations with training, fine-tuning, and scaling access to LLMs. NetApp® FlexCache® is a remote caching capability that simplifies file distribution and caches only the actively read data. This can be useful for LLM training, re-training, and fine tuning, bringing value to customers with business requirements like real-time rendering and LLM inferencing. NetApp® SnapMirror is an ONTAP feature that replicates volume snapshots between any two ONTAP systems. This feature optimally transfers data at the edge to your on-premises data center or to the cloud. SnapMirror can be used for moving data securely and efficiently between on-premises and hyperscaler clouds, when customers want to develop generative AI in clouds with RAG containing enterprise data."", ""metadata"": {""chunk_id"": 14, ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""source_type"": ""html""}, ""score"": 0.006818181818181818, ""rerank_score"": -6.0241827964782715}]"
3,What is the role of Lenovo NVIDIA OVX in NetApp AI Pod architecture?,The provided context does not mention the role of Lenovo NVIDIA OVX in NetApp AI Pod architecture.,['[Generative_AI_and_NetApp_Value.html]'],0.0,0.0,1.0,True,1,98,0.6,0.38,hybrid,21.148,"[{""content"": ""Data storage requirements should include integrity checks and immutable snapshots for structured, semi-structured, and unstructured data. NetApp Snapshots and SnapLock are being used for dataset versioning. It brings strict role-based access control (RBAC), as well as secure protocols, and industry standard encryption for securing both data at rest and in transit. Cloud Insights and Cloud Data Sense together offer capabilities to help you forensically identify the source of the threat and prioritize which data to restore. ONTAP AI with DGX BasePOD NetApp® ONTAP® AI reference architecture with NVIDIA DGX BasePOD is a scalable architecture for machine learning (ML) and artificial intelligence (AI) workloads. For the critical training phase of LLMs, data is typically copied from the data storage into the training cluster at regular intervals. The servers that are used in this phase use GPUs to parallelize computations, creating a tremendous appetite for data. Meeting the raw I/O bandwidth needs is crucial for maintaining high GPU utilization. ONTAP AI with NVIDIA AI Enterprise NVIDIA AI Enterprise is an end-to-end, cloud-native suite of AI and data analytics software that is optimized, certified, and supported by NVIDIA to run on VMware vSphere with NVIDIA-Certified Systems. This software facilitates the simple and rapid deployment, management, and scaling of AI workloads in the modern hybrid cloud environment."", ""metadata"": {""source_type"": ""html"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""chunk_id"": 17, ""doc_id"": ""6ae84725f8f4dd1a""}, ""score"": 0.009836065573770491, ""rerank_score"": 0.892216145992279}, {""content"": ""NVIDIA AI Enterprise, powered by NetApp and VMware, delivers enterprise-class AI workload and data management in a simplified, familiar package. 1P Cloud Platforms Fully managed cloud storage offerings are available natively on Microsoft Azure as Azure NetApp Files (ANF), on AWS as Amazon FSx for NetApp ONTAP (FSx ONTAP), and on Google as Google Cloud NetApp Volumes (GNCV). 1P is a managed, high-performance file system that enables customers to run highly available AI workloads with improved data security in public clouds, for fine-tuning LLMs/FMs with cloud native ML platforms like AWS SageMaker, Azure-OpenAI Services, and Google’s Vertex AI. NetApp Partner Solution Suite In addition to its core data products, technologies and capabilities, NetApp also collaborates closely with a robust network of AI partners to bring added value to customers. NVIDIA Guardrails in AI systems serve as safeguards to ensure the ethical and responsible use of AI technologies. AI developers can choose to define the behavior of LLM-powered applications on specific topics and prevent them from engaging in discussions on unwanted topics. Guardrails, an open-source toolkit, provides the ability to connect an LLM to other services, seamlessly and securely for building trustworthy, safe, and secure LLM conversational systems. Domino Data Lab provides versatile, enterprise-grade tools for building and productizing Generative AI - fast, safe, and economical, wherever you are in your AI journey."", ""metadata"": {""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 18, ""source_type"": ""html""}, ""score"": 0.00967741935483871, ""rerank_score"": -1.403808832168579}, {""content"": ""Generative AI solutions like LLMs need to read and process their source datasets from storage into memory numerous times to foster intelligence. NetApp has been a leader in data mobility, data governance and data security technologies across the edge-to-core-to-cloud ecosystem, serving enterprise customers build at-scale AI solutions. NetApp, with a strong network of partners has been helping chief data officers, AI engineers, enterprise architects and data scientists in the design of a free-flowing data pipeline for data preparation, data protection, and strategic data management responsibilities of AI model training and inferencing, optimizing the performance and scalability of the AI/ML lifecycle. NetApp data technologies and capabilities such as NetApp® ONTAP AI® for deep learning data pipeline, NetApp® SnapMirror® for transporting data seamlessly and efficiently between storage endpoints, and NetApp® FlexCache® for real-time rendering when the data flow shifts from batch to real-time and data engineering happens at prompt time, bring value to the deployment of real-time Generative AI models. As enterprises of all types embrace new AI tools, they face data challenges from the edge to the data center to the cloud that demand for scalable, responsible and explainable AI solutions. As the data authority on hybrid and multi cloud, NetApp is committed to building a network of partners and joint solutions that can help with all aspects of constructing a data pipeline and data lakes for generative AI model training (pre-training), fine-tuning, context-based inferencing and model decay monitoring of LLMs."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""source_type"": ""html"", ""chunk_id"": 2}, ""score"": 0.009523809523809523, ""rerank_score"": -3.5141472816467285}, {""content"": ""NetApp DataOps Toolkit The NetApp DataOps Toolkit is a Python-based tool that simplifies the management of development/training workspaces and inference servers that are backed by high-performance, scale-out NetApp storage. Key capabilities include: - Rapidly provision new high-capacity JupyterLab workspaces that are backed by high-performance, scale-out NetApp storage. - Rapidly provision new NVIDIA Triton Inference Server instances that are backed by enterprise-class NetApp storage. - Near-instantaneously clone high-capacity JupyterLab workspaces in order to enable experimentation or rapid iteration. - Near-instantaneously save snapshots of high-capacity JupyterLab workspaces for backup and/or traceability/baselining. - Near-instantaneously provision, clone, and snapshot high-capacity, high-performance data volumes."", ""metadata"": {""doc_id"": ""fc9d8e6a546b1e11"", ""chunk_id"": 0, ""file_name"": ""NetApp_DataOp_Toolkit.html"", ""source_type"": ""html""}, ""score"": 0.00909090909090909, ""rerank_score"": -4.704836368560791}, {""content"": ""Inaccurate outcomes or biased results generated by the models can put both businesses and consumers in jeopardy. These risks can correspond to constraints for LLMs emerging potentially from data management challenges associated with data quality, data security, and data mobility. NetApp helps organizations meet the complexities created by rapid data growth, data mobility, multi-cloud management, and the adoption of AI. At scale AI infrastructure and efficient data management is crucial to defining the success of AI applications like generative AI. It is critical customers cover all the deployment scenarios without compromising on the ability to expand as enterprises need to while maintaining cost-efficiency, data governance and ethical AI practices in control. NetApp is constantly working to help customers simplify and accelerate their AI deployments."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 20, ""source_type"": ""html"", ""file_name"": ""Generative_AI_and_NetApp_Value.html""}, ""score"": 0.008695652173913044, ""rerank_score"": -4.807443618774414}, {""content"": ""Vector Database This section covers the definition and use of a vector database in NetApp AI solutions. Vector Database A vector database is a specialized type of database designed to handle, index, and search unstructured data using embeddings from machine learning models. Instead of organizing data in a traditional tabular format, it arranges data as high-dimensional vectors, also known as vector embeddings. This unique structure allows the database to handle complex, multi-dimensional data more efficiently and accurately. One of the key capabilities of a vector database is its use of generative AI to perform analytics. This includes similarity searches, where the database identifies data points that are like a given input, and anomaly detection, where it can spot data points that deviate significantly from the norm. Furthermore, vector databases are well-suited to handle temporal data, or time-stamped data. This type of data provides information about ‘what’ happened and when it happened, in sequence and in relation to all other events within a given IT system. This ability to handle and analyze temporal data makes vector databases particularly useful for applications that require an understanding of events over time. Advantages of vector database for ML and AI: - High-dimensional Search: Vector databases excel in managing and retrieving high-dimensional data, which is often generated in AI and ML applications."", ""metadata"": {""chunk_id"": 0, ""doc_id"": ""abdaba3d5a02bafe"", ""file_name"": ""Vector_Database.html"", ""source_type"": ""html""}, ""score"": 0.007058823529411764, ""rerank_score"": -6.70945930480957}, {""content"": ""* Storage: it’s responsible for data persistence. It comprises meta storage, log broker, and object storage. NetApp storage such as ONTAP and StorageGRID provides object storage and File based storage to Milvus for both customer data and vector database data."", ""metadata"": {""chunk_id"": 6, ""source_type"": ""html"", ""file_name"": ""Vector_Database.html"", ""doc_id"": ""abdaba3d5a02bafe""}, ""score"": 0.008108108108108109, ""rerank_score"": -7.043979644775391}, {""content"": ""As a result, phrases like Stable Diffusion, Hallucinations, Prompt Engineering and Value Alignment are rapidly emerging in the design of AI systems. These self-supervised or semi-supervised machine learning (ML) models are becoming widely available as pre-trained foundation models (FM) via cloud service providers and other AI firmsvendors, which are being adopted by various business establishments across industries for a wide range of downstream NLP (natural language processing) tasks. As asserted by research analyst firms like McKinsey – “Generative AI’s impact on productivity could add trillions of dollars in value to the global economy.” While companies are reimagining AI as thought partners to humans and FMs are broadening simultaneously to what businesses and institutions can do with generative AI, the opportunities to manage massive volumes of data will continue to grow. This document presents introductory information on generative AI and the design concepts in relation to NetApp capabilities that bring value to NetApp customers, both on-premises and hybrid or multicloud environments. So, what’s in it for customers to use NetApp in their AI environments? NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI. NetApp has combined various capabilities into intelligent data management software and storage infrastructure that have been well balanced with high-performance optimized for AI workloads."", ""metadata"": {""source_type"": ""html"", ""chunk_id"": 1, ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""doc_id"": ""6ae84725f8f4dd1a""}, ""score"": 0.008333333333333333, ""rerank_score"": -7.846642017364502}]"
4,Explain the features of NetApp DataOp Toolkit.,"The NetApp DataOps Toolkit is a Python-based tool that simplifies the management of development/training workspaces and inference servers backed by high-performance, scale-out NetApp storage. Its key capabilities include rapidly provisioning new high-capacity JupyterLab workspaces and NVIDIA Triton Inference Server instances, near-instantaneously cloning and snapshotting high-capacity JupyterLab workspaces, and near-instantaneously provisioning, cloning, and snapshotting high-capacity, high-performance data volumes.",['[NetApp_DataOp_Toolkit.html]'],1.0,0.0,0.0,True,1,521,1.0,0.42,vector,7.054,"[{""content"": ""NetApp DataOps Toolkit The NetApp DataOps Toolkit is a Python-based tool that simplifies the management of development/training workspaces and inference servers that are backed by high-performance, scale-out NetApp storage. Key capabilities include: - Rapidly provision new high-capacity JupyterLab workspaces that are backed by high-performance, scale-out NetApp storage. - Rapidly provision new NVIDIA Triton Inference Server instances that are backed by enterprise-class NetApp storage. - Near-instantaneously clone high-capacity JupyterLab workspaces in order to enable experimentation or rapid iteration. - Near-instantaneously save snapshots of high-capacity JupyterLab workspaces for backup and/or traceability/baselining. - Near-instantaneously provision, clone, and snapshot high-capacity, high-performance data volumes."", ""metadata"": {""chunk_id"": 0, ""source_type"": ""html"", ""doc_id"": ""fc9d8e6a546b1e11"", ""file_name"": ""NetApp_DataOp_Toolkit.html""}, ""score"": 0.3414064645767212}, {""content"": ""This offers flexibility to customers who need easy access to heterogenous data sources with the ability to share data from both NFS and S3. For e.g., fine-tuning FMs like Meta’s Llama 2 text-generation models on SageMaker with access to file-object buckets. NetApp® Cloud Sync service offers a simple and secure way to migrate data to any target, in the cloud or on-premises. Cloud Sync seamlessly transfers and synchronizes data between on-premises or cloud storage, NAS, and object stores. NetApp XCP is a client software that enables fast and reliable any-to-NetApp and NetApp-to-NetApp data migrations. XCP also provides the capability of moving bulk data efficiently from Hadoop HDFS file systems into ONTAP NFS, S3 or StorageGRID and XCP file analytics provides visibility into the file system. NetApp® DataOps Toolkit is a Python library that makes it simple for data scientists, DevOps, and data engineers to perform various data management tasks, such as near-instantaneously provisioning, cloning, or snapshotting a data volume or JupyterLab workspace that are backed by high-performance scale-out NetApp storage. NetApp’s product security. LLMs may inadvertently reveal confidential data in their responses, thus a concern to CISOs who study the vulnerabilities associated with AI applications leveraging LLMs. As outlined by OWASP (Open Worldwide Application Security Project), security issues such as data poisoning, data leakage, denial of service and prompt injections within LLMs can impact businesses from data exposure to unauthorized access serving attackers."", ""metadata"": {""source_type"": ""html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 16, ""file_name"": ""Generative_AI_and_NetApp_Value.html""}, ""score"": 0.01595604419708252}, {""content"": ""It efficiently transfers only changes, saving bandwidth and speeding replication, thus bringing essential data mobility features during the operations of training, re-training, and fine-tuning of FMs or LLMs. NetApp® SnapLock brings immutable disk capability on ONTAP-based storage systems for dataset versioning. The microcore architecture is designed to protect customer data with FPolicyTM Zero Trust engine. NetApp ensures customer data is available by resisting denial-of-service (DoS) attacks when an attacker interacts with an LLM in a particularly resource-consuming way. NetApp® Cloud Data Sense helps identify, map, and classify personal information present in enterprise datasets, enact policies, meet privacy requirements on premises or in the cloud, help improve security posture and comply with regulations. NetApp® BlueXPTM classification, powered by Cloud Data Sense. Customers can automatically scan, analyze, categorize, and act on data across data estate, detect security risks, optimize storage, and accelerate cloud deployments. It combines storage and data services via its unified control plane, Customers can use GPU instances for computation, and hybrid multicloud environments for cold storage tiering and for archives and backups. NetApp File-Object Duality. NetApp ONTAP enables dual-protocol access for NFS and S3. With this solution, customers can access NFS data from Amazon AWS SageMaker notebooks via S3 buckets from NetApp Cloud Volumes ONTAP."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""source_type"": ""html"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""chunk_id"": 15}, ""score"": 0.009315967559814453}, {""content"": ""ONTAP includes various data management and protection features and capabilities, including automatic ransomware protection against cyber-attacks, built-in data transport features, and storage efficiency capabilities for a range of architectures from on-premises, hybrid, multiclouds in NAS, SAN, object, and software defined storage (SDS) situations of LLM deployments. NetApp® ONTAP AI® for deep learning model training. NetApp® ONTAP® supports NVIDIA GPU Direct StorageTM with the use of NFS over RDMA for NetApp customers with ONTAP storage cluster and NVIDIA DGX compute nodes . It offers a cost-efficient performance to read and process source datasets from storage into memory numerous times to foster intelligence, enabling organizations with training, fine-tuning, and scaling access to LLMs. NetApp® FlexCache® is a remote caching capability that simplifies file distribution and caches only the actively read data. This can be useful for LLM training, re-training, and fine tuning, bringing value to customers with business requirements like real-time rendering and LLM inferencing. NetApp® SnapMirror is an ONTAP feature that replicates volume snapshots between any two ONTAP systems. This feature optimally transfers data at the edge to your on-premises data center or to the cloud. SnapMirror can be used for moving data securely and efficiently between on-premises and hyperscaler clouds, when customers want to develop generative AI in clouds with RAG containing enterprise data."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""chunk_id"": 14, ""source_type"": ""html"", ""file_name"": ""Generative_AI_and_NetApp_Value.html""}, ""score"": -0.012207865715026855}, {""content"": ""Data storage requirements should include integrity checks and immutable snapshots for structured, semi-structured, and unstructured data. NetApp Snapshots and SnapLock are being used for dataset versioning. It brings strict role-based access control (RBAC), as well as secure protocols, and industry standard encryption for securing both data at rest and in transit. Cloud Insights and Cloud Data Sense together offer capabilities to help you forensically identify the source of the threat and prioritize which data to restore. ONTAP AI with DGX BasePOD NetApp® ONTAP® AI reference architecture with NVIDIA DGX BasePOD is a scalable architecture for machine learning (ML) and artificial intelligence (AI) workloads. For the critical training phase of LLMs, data is typically copied from the data storage into the training cluster at regular intervals. The servers that are used in this phase use GPUs to parallelize computations, creating a tremendous appetite for data. Meeting the raw I/O bandwidth needs is crucial for maintaining high GPU utilization. ONTAP AI with NVIDIA AI Enterprise NVIDIA AI Enterprise is an end-to-end, cloud-native suite of AI and data analytics software that is optimized, certified, and supported by NVIDIA to run on VMware vSphere with NVIDIA-Certified Systems. This software facilitates the simple and rapid deployment, management, and scaling of AI workloads in the modern hybrid cloud environment."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""chunk_id"": 17, ""source_type"": ""html""}, ""score"": -0.03299367427825928}, {""content"": ""Generative AI solutions like LLMs need to read and process their source datasets from storage into memory numerous times to foster intelligence. NetApp has been a leader in data mobility, data governance and data security technologies across the edge-to-core-to-cloud ecosystem, serving enterprise customers build at-scale AI solutions. NetApp, with a strong network of partners has been helping chief data officers, AI engineers, enterprise architects and data scientists in the design of a free-flowing data pipeline for data preparation, data protection, and strategic data management responsibilities of AI model training and inferencing, optimizing the performance and scalability of the AI/ML lifecycle. NetApp data technologies and capabilities such as NetApp® ONTAP AI® for deep learning data pipeline, NetApp® SnapMirror® for transporting data seamlessly and efficiently between storage endpoints, and NetApp® FlexCache® for real-time rendering when the data flow shifts from batch to real-time and data engineering happens at prompt time, bring value to the deployment of real-time Generative AI models. As enterprises of all types embrace new AI tools, they face data challenges from the edge to the data center to the cloud that demand for scalable, responsible and explainable AI solutions. As the data authority on hybrid and multi cloud, NetApp is committed to building a network of partners and joint solutions that can help with all aspects of constructing a data pipeline and data lakes for generative AI model training (pre-training), fine-tuning, context-based inferencing and model decay monitoring of LLMs."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""source_type"": ""html"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""chunk_id"": 2}, ""score"": -0.07555186748504639}, {""content"": ""NVIDIA AI Enterprise, powered by NetApp and VMware, delivers enterprise-class AI workload and data management in a simplified, familiar package. 1P Cloud Platforms Fully managed cloud storage offerings are available natively on Microsoft Azure as Azure NetApp Files (ANF), on AWS as Amazon FSx for NetApp ONTAP (FSx ONTAP), and on Google as Google Cloud NetApp Volumes (GNCV). 1P is a managed, high-performance file system that enables customers to run highly available AI workloads with improved data security in public clouds, for fine-tuning LLMs/FMs with cloud native ML platforms like AWS SageMaker, Azure-OpenAI Services, and Google’s Vertex AI. NetApp Partner Solution Suite In addition to its core data products, technologies and capabilities, NetApp also collaborates closely with a robust network of AI partners to bring added value to customers. NVIDIA Guardrails in AI systems serve as safeguards to ensure the ethical and responsible use of AI technologies. AI developers can choose to define the behavior of LLM-powered applications on specific topics and prevent them from engaging in discussions on unwanted topics. Guardrails, an open-source toolkit, provides the ability to connect an LLM to other services, seamlessly and securely for building trustworthy, safe, and secure LLM conversational systems. Domino Data Lab provides versatile, enterprise-grade tools for building and productizing Generative AI - fast, safe, and economical, wherever you are in your AI journey."", ""metadata"": {""chunk_id"": 18, ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""source_type"": ""html""}, ""score"": -0.1024019718170166}, {""content"": ""* Storage: it’s responsible for data persistence. It comprises meta storage, log broker, and object storage. NetApp storage such as ONTAP and StorageGRID provides object storage and File based storage to Milvus for both customer data and vector database data."", ""metadata"": {""chunk_id"": 6, ""file_name"": ""Vector_Database.html"", ""doc_id"": ""abdaba3d5a02bafe"", ""source_type"": ""html""}, ""score"": -0.11005842685699463}]"
5,What are the installation steps for IBM Spectrum Scale with NetApp E-Series storage?,"The installation steps for IBM Spectrum Scale with NetApp E-Series storage are described in TR-4859. The process involves installing Spectrum Scale, validating the infrastructure, and managing the configuration.",['[TR_4859_Deploying_IBM_spectrum_scale_wit_NetApp_E_Series_storage_Installation_and_validation.html]'],0.9,0.0,1.0,True,1,211,1.0,0.555,hybrid,10.396,"[{""content"": ""TR-4859: Deploying IBM spectrum scale with NetApp E-Series storage - Installation and validation Suggest changes Chris Seirer, NetApp TR-4859 describes the process of deploying a full parallel file system solution based on IBM’s Spectrum Scale software stack. TR-4859 is designed to provide details on how to install Spectrum Scale, validate the infrastructure, and manage the configuration."", ""metadata"": {""chunk_id"": 0, ""doc_id"": ""1ba0d082be2942b2"", ""file_name"": ""TR_4859_Deploying_IBM_spectrum_scale_wit_NetApp_E_Series_storage_Installation_and_validation.html"", ""source_type"": ""html""}, ""score"": 0.009836065573770491, ""rerank_score"": 6.854271411895752}, {""content"": ""NetApp DataOps Toolkit The NetApp DataOps Toolkit is a Python-based tool that simplifies the management of development/training workspaces and inference servers that are backed by high-performance, scale-out NetApp storage. Key capabilities include: - Rapidly provision new high-capacity JupyterLab workspaces that are backed by high-performance, scale-out NetApp storage. - Rapidly provision new NVIDIA Triton Inference Server instances that are backed by enterprise-class NetApp storage. - Near-instantaneously clone high-capacity JupyterLab workspaces in order to enable experimentation or rapid iteration. - Near-instantaneously save snapshots of high-capacity JupyterLab workspaces for backup and/or traceability/baselining. - Near-instantaneously provision, clone, and snapshot high-capacity, high-performance data volumes."", ""metadata"": {""chunk_id"": 0, ""file_name"": ""NetApp_DataOp_Toolkit.html"", ""doc_id"": ""fc9d8e6a546b1e11"", ""source_type"": ""html""}, ""score"": 0.008823529411764706, ""rerank_score"": -1.5269170999526978}, {""content"": ""NetApp recommends implementing such a management tool to achieve fast (under a minute) snapshot backup, quick (minutes) database restore, and database clone. Solution deployment The following sections provide step-by-step procedures for automated Oracle 19c deployment and database migration on Azure NetApp Files with directly mounted database volumes via NFS to Azure VMs. Prerequisites for deployment Details Deployment requires the following prerequisites. - An Azure account has been set up, and the necessary VNet and network segments have been created within your Azure account. - From the Azure cloud portal, deploy Azure Linux VMs as Oracle DB servers. Create an Azure NetApp Files capacity pool and database volumes for Oracle database. Enable VM SSH private/public key authentication for azureuser to DB servers. See the architecture diagram in the previous section for details about the environment setup. Also referred to Step-by-Step Oracle deployment procedures on Azure VM and Azure NetApp Files for detailed information. For Azure VMs deployed with local disk redundancy, ensure that you have allocated at least 128G in the VM root disk to have sufficient space to stage Oracle installation files and add OS swap file. Expand /tmplv and /rootlv OS partition accordingly. Add 1G free space to rootvg-homelv if it is less than 1G."", ""metadata"": {""source_type"": ""html"", ""doc_id"": ""71b5e79c7e587b3f"", ""chunk_id"": 4, ""file_name"": ""TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html""}, ""score"": 0.00967741935483871, ""rerank_score"": -3.839529275894165}, {""content"": ""It efficiently transfers only changes, saving bandwidth and speeding replication, thus bringing essential data mobility features during the operations of training, re-training, and fine-tuning of FMs or LLMs. NetApp® SnapLock brings immutable disk capability on ONTAP-based storage systems for dataset versioning. The microcore architecture is designed to protect customer data with FPolicyTM Zero Trust engine. NetApp ensures customer data is available by resisting denial-of-service (DoS) attacks when an attacker interacts with an LLM in a particularly resource-consuming way. NetApp® Cloud Data Sense helps identify, map, and classify personal information present in enterprise datasets, enact policies, meet privacy requirements on premises or in the cloud, help improve security posture and comply with regulations. NetApp® BlueXPTM classification, powered by Cloud Data Sense. Customers can automatically scan, analyze, categorize, and act on data across data estate, detect security risks, optimize storage, and accelerate cloud deployments. It combines storage and data services via its unified control plane, Customers can use GPU instances for computation, and hybrid multicloud environments for cold storage tiering and for archives and backups. NetApp File-Object Duality. NetApp ONTAP enables dual-protocol access for NFS and S3. With this solution, customers can access NFS data from Amazon AWS SageMaker notebooks via S3 buckets from NetApp Cloud Volumes ONTAP."", ""metadata"": {""doc_id"": ""6ae84725f8f4dd1a"", ""source_type"": ""html"", ""chunk_id"": 15, ""file_name"": ""Generative_AI_and_NetApp_Value.html""}, ""score"": 0.00923076923076923, ""rerank_score"": -3.9973630905151367}, {""content"": ""* Storage: it’s responsible for data persistence. It comprises meta storage, log broker, and object storage. NetApp storage such as ONTAP and StorageGRID provides object storage and File based storage to Milvus for both customer data and vector database data."", ""metadata"": {""doc_id"": ""abdaba3d5a02bafe"", ""source_type"": ""html"", ""chunk_id"": 6, ""file_name"": ""Vector_Database.html""}, ""score"": 0.008955223880597015, ""rerank_score"": -4.81614875793457}, {""content"": ""NVIDIA AI Enterprise, powered by NetApp and VMware, delivers enterprise-class AI workload and data management in a simplified, familiar package. 1P Cloud Platforms Fully managed cloud storage offerings are available natively on Microsoft Azure as Azure NetApp Files (ANF), on AWS as Amazon FSx for NetApp ONTAP (FSx ONTAP), and on Google as Google Cloud NetApp Volumes (GNCV). 1P is a managed, high-performance file system that enables customers to run highly available AI workloads with improved data security in public clouds, for fine-tuning LLMs/FMs with cloud native ML platforms like AWS SageMaker, Azure-OpenAI Services, and Google’s Vertex AI. NetApp Partner Solution Suite In addition to its core data products, technologies and capabilities, NetApp also collaborates closely with a robust network of AI partners to bring added value to customers. NVIDIA Guardrails in AI systems serve as safeguards to ensure the ethical and responsible use of AI technologies. AI developers can choose to define the behavior of LLM-powered applications on specific topics and prevent them from engaging in discussions on unwanted topics. Guardrails, an open-source toolkit, provides the ability to connect an LLM to other services, seamlessly and securely for building trustworthy, safe, and secure LLM conversational systems. Domino Data Lab provides versatile, enterprise-grade tools for building and productizing Generative AI - fast, safe, and economical, wherever you are in your AI journey."", ""metadata"": {""source_type"": ""html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""chunk_id"": 18}, ""score"": 0.00821917808219178, ""rerank_score"": -5.545721054077148}, {""content"": ""Inaccurate outcomes or biased results generated by the models can put both businesses and consumers in jeopardy. These risks can correspond to constraints for LLMs emerging potentially from data management challenges associated with data quality, data security, and data mobility. NetApp helps organizations meet the complexities created by rapid data growth, data mobility, multi-cloud management, and the adoption of AI. At scale AI infrastructure and efficient data management is crucial to defining the success of AI applications like generative AI. It is critical customers cover all the deployment scenarios without compromising on the ability to expand as enterprises need to while maintaining cost-efficiency, data governance and ethical AI practices in control. NetApp is constantly working to help customers simplify and accelerate their AI deployments."", ""metadata"": {""file_name"": ""Generative_AI_and_NetApp_Value.html"", ""chunk_id"": 20, ""source_type"": ""html"", ""doc_id"": ""6ae84725f8f4dd1a""}, ""score"": 0.007792207792207792, ""rerank_score"": -6.140384197235107}, {""content"": ""Sensitive data that gets passed to FMs or LLMs may include PII, financial information, health information, biometric data, location data, communications data, online behavior, and legal information. In such an event of real-time rendering, prompt execution and edge inferencing there is data movement from end user app to storage endpoints via open source or proprietary LLM models to a data center on premises or public cloud platforms. In all such scenarios, data mobility and data protection are crucial for the AI operations involving LLMs which rely on large training datasets and movement of such data. Figure 4: Generative AI - LLM Data Pipeline NetApp's portfolio of storage infrastructure, data and cloud services is powered by intelligent data management software. Data Preparation: The first pillar of the LLM tech stack is largely untouched from the older traditional ML stack. Data preprocessing in AI pipeline is necessary to normalize and cleanse the data before training or fine-tuning. This step includes connectors to ingest data wherever it may reside in the form of an Amazon S3 tier or in on-premises storage systems such as a file store or an object store like NetApp StorageGRID. NetApp® ONTAP is the foundational technology that underpins NetApp's critical storage solutions in the data center and the cloud."", ""metadata"": {""chunk_id"": 13, ""source_type"": ""html"", ""doc_id"": ""6ae84725f8f4dd1a"", ""file_name"": ""Generative_AI_and_NetApp_Value.html""}, ""score"": 0.007692307692307692, ""rerank_score"": -6.657380104064941}]"
6,Describe the automated Oracle deployment on Azure NetApp Files with NFS.,"The automated Oracle deployment on Azure NetApp Files with NFS is a solution that simplifies the deployment of Oracle databases in Azure NetApp Files via NFS mounts using Ansible automation. The Oracle database deploys in a container database (CDB) and pluggable databases (PDB) configuration with Oracle dNFS protocol enabled to boost performance. The solution provides step-by-step procedures for automated Oracle 19c deployment and database migration on Azure NetApp Files with directly mounted database volumes via NFS to Azure VMs. The volumes are mounted on Oracle DB server as /u01 - binary, /u02 - data, /u03 - logs via NFS. Dual control files are configured on /u02 and /u03 mount points for redundancy. The automation solution can deploy an Oracle container database to multiple DB servers in a single Ansible playbook run. By using dNFS (available since Oracle 11g), an Oracle database running on an Azure Virtual Machine can drive significantly more I/O than the native NFS client. NetApp provides a SnapCenter software suite for database backup, restore, and cloning with a user-friendly UI interface.",['[TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html:1]'],0.9,0.0,1.0,True,1,1114,1.0,0.555,hybrid,8.21,"[{""content"": ""TR-4987: Simplified, Automated Oracle Deployment on Azure NetApp Files with NFS Allen Cao, Niyaz Mohamed, NetApp This solution provides overview and details for automated Oracle deployment in Microsoft Azure NetApp Files as primary database storage with NFS protocol and Oracle database is deployed as container database with dNFS enabled. Purpose Running performance-intensive and latency-sensitive Oracle workloads in the cloud can be challenging. Azure NetApp Files (ANF) makes it easy for enterprise line-of-business (LOB) and storage professionals to migrate and run demanding Oracle workloads without code change. Azure NetApp Files is widely used as the underlying shared file-storage service in various scenarios, such as new deployment or migration (lift and shift) of Oracle databases from on-premises to Azure. This documentation demonstrates the simplified deployment of Oracle databases in Azure NetApp files via NFS mounts using Ansible automation. The Oracle database deploys in a container database (CDB) and pluggable databases (PDB) configuration with Oracle dNFS protocol enabled to boost performance. Furthermore, the on-premises Oracle single instance database or PDB can be migrated into a newly deployed container database in Azure using automated PDB relocation methodology with minimal service interruption. It also provides information on fast Oracle database backup, restore, and clone with NetApp SnapCenter UI tool in Azure Cloud."", ""metadata"": {""source_type"": ""html"", ""doc_id"": ""71b5e79c7e587b3f"", ""chunk_id"": 0, ""file_name"": ""TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html""}, ""score"": 0.009836065573770491, ""rerank_score"": 9.248126983642578}, {""content"": ""NetApp recommends implementing such a management tool to achieve fast (under a minute) snapshot backup, quick (minutes) database restore, and database clone. Solution deployment The following sections provide step-by-step procedures for automated Oracle 19c deployment and database migration on Azure NetApp Files with directly mounted database volumes via NFS to Azure VMs. Prerequisites for deployment Details Deployment requires the following prerequisites. - An Azure account has been set up, and the necessary VNet and network segments have been created within your Azure account. - From the Azure cloud portal, deploy Azure Linux VMs as Oracle DB servers. Create an Azure NetApp Files capacity pool and database volumes for Oracle database. Enable VM SSH private/public key authentication for azureuser to DB servers. See the architecture diagram in the previous section for details about the environment setup. Also referred to Step-by-Step Oracle deployment procedures on Azure VM and Azure NetApp Files for detailed information. For Azure VMs deployed with local disk redundancy, ensure that you have allocated at least 128G in the VM root disk to have sufficient space to stage Oracle installation files and add OS swap file. Expand /tmplv and /rootlv OS partition accordingly. Add 1G free space to rootvg-homelv if it is less than 1G."", ""metadata"": {""source_type"": ""html"", ""file_name"": ""TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html"", ""doc_id"": ""71b5e79c7e587b3f"", ""chunk_id"": 4}, ""score"": 0.009523809523809523, ""rerank_score"": 7.094829559326172}, {""content"": ""Ensure the database volume naming follows the VMname-u01, VMname-u02, and VMname-u03 convention. sudo lvresize -r -L +20G /dev/mapper/rootvg-rootlv sudo lvresize -r -L +10G /dev/mapper/rootvg-tmplv sudo lvresize -r -L +1G /dev/mapper/rootvg-homelv - From the Azure cloud portal, provision a Windows server to run the NetApp SnapCenter UI tool with the latest version. Refer to the following link for details: Install the SnapCenter Server - Provision a Linux VM as the Ansible controller node with the latest version of Ansible and Git installed. Refer to the following link for details: Getting Started with NetApp solution automation in section - Setup the Ansible Control Node for CLI deployments on RHEL / CentOS or Setup the Ansible Control Node for CLI deployments on Ubuntu / Debian .The Ansible controller node can locate either on-premisses or in Azure cloud as far as it can reach Azure DB VMs via ssh port. - Clone a copy of the NetApp Oracle deployment automation toolkit for NFS. git clone - Stage following Oracle 19c installation files on Azure DB VM /tmp/archive directory with 777 permission. installer_archives: - \""LINUX.X64_193000_db_home.zip\"" - \""p34765931_190000_Linux-x86-64.zip\"" - \""p6880880_190000_Linux-x86-64.zip\"" - Watch the following video: Simplified and automated Oracle deployment on Azure NetApp Files with NFS Automation parameter files Details Ansible playbook executes database installation and configuration tasks with predefined parameters."", ""metadata"": {""chunk_id"": 5, ""source_type"": ""html"", ""file_name"": ""TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html"", ""doc_id"": ""71b5e79c7e587b3f""}, ""score"": 0.00923076923076923, ""rerank_score"": 5.605612754821777}, {""content"": ""The volumes are mounted on Oracle DB server as /u01 - binary, /u02 - data, /u03 - logs via NFS. Dual control files are configured on /u02 and /u03 mount points for redundancy. - Multiple DB servers deployment. The automation solution can deploy an Oracle container database to multiple DB servers in a single Ansible playbook run. Regardless of the number of DB servers, the playbook execution remains the same. You can deploy multiple container databases to a single VM instance by repeating the deployment with different database instance IDs (Oracle SID). But ensure there is sufficient memory on the host to support deployed databases. - dNFS configuration. By using dNFS (available since Oracle 11g), an Oracle database running on an Azure Virtual Machine can drive significantly more I/O than the native NFS client. Automated Oracle deployment configures dNFS on NFSv3 by default. - Allocate large size volume to speed up deployment. ANF file system IO throughput is regulated based on the size of volume. For initial deployment, allocate large size volumes can speed up the deployment. The volumes subsequently can be downsized dynamically without application impact. - Database backup. NetApp provides a SnapCenter software suite for database backup, restore, and cloning with a user-friendly UI interface."", ""metadata"": {""source_type"": ""html"", ""chunk_id"": 3, ""file_name"": ""TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html"", ""doc_id"": ""71b5e79c7e587b3f""}, ""score"": 0.009375, ""rerank_score"": 5.405694007873535}, {""content"": ""This solution addresses the following use cases: - Automated Oracle container database deployment on Azure NetApp files - Automated Oracle database migration between on-premisses and Azure cloud Audience This solution is intended for the following people: - A DBA who would like to deploy Oracle on Azure NetApp Files. - A database solution architect who would like to test Oracle workloads on Azure NetApp Files. - A storage administrator who would like to deploy and manage an Oracle database on Azure NetApp Files. - An application owner who would like to stand up an Oracle database on Azure NetApp Files. Solution test and validation environment The testing and validation of this solution were performed in a lab setting that might not match the final deployment environment. See the section Key factors for deployment consideration for more information."", ""metadata"": {""chunk_id"": 1, ""doc_id"": ""71b5e79c7e587b3f"", ""source_type"": ""html"", ""file_name"": ""TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html""}, ""score"": 0.00967741935483871, ""rerank_score"": 5.087479591369629}, {""content"": ""Architecture Hardware and software components Hardware | || Azure NetApp Files | Current offering in Azure by Microsoft | One capacity pool with Premium service level | Azure VM for DB server | Standard_B4ms - 4 vCPUs, 16GiB | Two Linux virtual machine instances for concurrent deployment | Azure VM for SnapCenter | Standard_B4ms - 4 vCPUs, 16GiB | One Windows virtual machine instance | Software | || RedHat Linux | RHEL Linux 8.6 (LVM) - x64 Gen2 | Deployed RedHat subscription for testing | Windows Server | 2022 DataCenter; Azure Edition Hotpatch - x64 Gen2 | Hosting SnapCenter server | Oracle Database | Version 19.18 | Applied RU patch p34765931_190000_Linux-x86-64.zip | Oracle OPatch | Version 12.2.0.1.36 | Latest patch p6880880_190000_Linux-x86-64.zip | SnapCenter Server | Version 5.0 | Workgroup deployment | Open JDK | Version java-11-openjdk | SnapCenter plugin requirement on DB VMs | NFS | Version 3.0 | Oracle dNFS enabled | Ansible | core 2.16.2 | Python 3.6.8 | Oracle database configuration in the lab environment Server | Database | DB Storage | ora-01 | NTAP1(NTAP1_PDB1,NTAP1_PDB2,NTAP1_PDB3) | /u01, /u02, /u03 NFS mounts on ANF capacity pool | ora-02 | NTAP2(NTAP2_PDB1,NTAP2_PDB2,NTAP2_PDB3) | /u01, /u02, /u03 NFS mounts on ANF capacity pool | Key factors for deployment consideration - Oracle database storage layout. In this automated Oracle deployment, we provision three database volumes for each database to host Oracle binary, data, and logs by default."", ""metadata"": {""source_type"": ""html"", ""file_name"": ""TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html"", ""chunk_id"": 2, ""doc_id"": ""71b5e79c7e587b3f""}, ""score"": 0.008955223880597015, ""rerank_score"": 4.686444282531738}, {""content"": ""NVIDIA AI Enterprise, powered by NetApp and VMware, delivers enterprise-class AI workload and data management in a simplified, familiar package. 1P Cloud Platforms Fully managed cloud storage offerings are available natively on Microsoft Azure as Azure NetApp Files (ANF), on AWS as Amazon FSx for NetApp ONTAP (FSx ONTAP), and on Google as Google Cloud NetApp Volumes (GNCV). 1P is a managed, high-performance file system that enables customers to run highly available AI workloads with improved data security in public clouds, for fine-tuning LLMs/FMs with cloud native ML platforms like AWS SageMaker, Azure-OpenAI Services, and Google’s Vertex AI. NetApp Partner Solution Suite In addition to its core data products, technologies and capabilities, NetApp also collaborates closely with a robust network of AI partners to bring added value to customers. NVIDIA Guardrails in AI systems serve as safeguards to ensure the ethical and responsible use of AI technologies. AI developers can choose to define the behavior of LLM-powered applications on specific topics and prevent them from engaging in discussions on unwanted topics. Guardrails, an open-source toolkit, provides the ability to connect an LLM to other services, seamlessly and securely for building trustworthy, safe, and secure LLM conversational systems. Domino Data Lab provides versatile, enterprise-grade tools for building and productizing Generative AI - fast, safe, and economical, wherever you are in your AI journey."", ""metadata"": {""source_type"": ""html"", ""chunk_id"": 18, ""doc_id"": ""6ae84725f8f4dd1a"", ""file_name"": ""Generative_AI_and_NetApp_Value.html""}, ""score"": 0.008450704225352112, ""rerank_score"": 0.6203103065490723}, {""content"": ""For this Oracle automation solution, there are three user-defined parameter files that need user input before playbook execution. - hosts - define targets that the automation playbook is running against. - vars/vars.yml - the global variable file that defines variables that apply to all targets. - host_vars/host_name.yml - the local variable file that defines variables that apply only to a named target. In our use case, these are the Oracle DB servers. In addition to these user-defined variable files, there are several default variable files that contain default parameters that do not require change unless necessary. The following sections show how to configure the user-defined variable files. Parameter files configuration Details - Ansible target hosts file configuration:# Enter Oracle servers names to be deployed one by one, follow by each Oracle server public IP address, and ssh private key of admin user for the server. [oracle] ora_01 ansible_host=10.61.180.21 ansible_ssh_private_key_file=ora_01.pem ora_02 ansible_host=10.61.180.23 ansible_ssh_private_key_file=ora_02.pem - Global vars/vars.yml file configuration###################################################################### ###### Oracle 19c deployment user configuration variables ###### ###### Consolidate all variables from ONTAP, linux and oracle ###### ###################################################################### ########################################### ### ONTAP env specific config variables ### ########################################### # Prerequisite to create three volumes in NetApp ONTAP storage from System Manager or cloud dashboard with following naming convention: # db_hostname_u01 - Oracle binary # db_hostname_u02 - Oracle data # db_hostname_u03 - Oracle redo # It is important to strictly follow the name convention or the automation will fail."", ""metadata"": {""file_name"": ""TR_4987_Simplified_Automated_Oracle_Deployment_on_Azure_NetApp_Files_with_NFS.html"", ""source_type"": ""html"", ""doc_id"": ""71b5e79c7e587b3f"", ""chunk_id"": 6}, ""score"": 0.008695652173913044, ""rerank_score"": -0.6284812688827515}]"
7,What is a vector database and why is it important in AI applications?,"A vector database is a specialized type of database designed to handle, index, and search unstructured data using embeddings from machine learning models. It arranges data as high-dimensional vectors, allowing it to handle complex, multi-dimensional data more efficiently and accurately. Vector databases are important in AI applications because they excel in managing and retrieving high-dimensional data, can efficiently scale to handle large volumes of data, offer high-performance data management and retrieval, and provide customizable indexing options. They are also well-suited to handle temporal data and can perform analytics such as similarity searches and anomaly detection.",['[Vector_Database.html:1]'],0.9,0.0,1.0,True,1,685,1.0,0.555,vector,8.475,"[{""content"": ""Vector Database This section covers the definition and use of a vector database in NetApp AI solutions. Vector Database A vector database is a specialized type of database designed to handle, index, and search unstructured data using embeddings from machine learning models. Instead of organizing data in a traditional tabular format, it arranges data as high-dimensional vectors, also known as vector embeddings. This unique structure allows the database to handle complex, multi-dimensional data more efficiently and accurately. One of the key capabilities of a vector database is its use of generative AI to perform analytics. This includes similarity searches, where the database identifies data points that are like a given input, and anomaly detection, where it can spot data points that deviate significantly from the norm. Furthermore, vector databases are well-suited to handle temporal data, or time-stamped data. This type of data provides information about ‘what’ happened and when it happened, in sequence and in relation to all other events within a given IT system. This ability to handle and analyze temporal data makes vector databases particularly useful for applications that require an understanding of events over time. Advantages of vector database for ML and AI: - High-dimensional Search: Vector databases excel in managing and retrieving high-dimensional data, which is often generated in AI and ML applications."", ""metadata"": {""source_type"": ""html"", ""file_name"": ""Vector_Database.html"", ""doc_id"": ""abdaba3d5a02bafe"", ""chunk_id"": 0}, ""score"": 0.23251831531524658}, {""content"": ""- Scalability: They can efficiently scale to handle large volumes of data, supporting the growth and expansion of AI and ML projects. - Flexibility: Vector databases offer a high degree of flexibility, allowing for the accommodation of diverse data types and structures. - Performance: They provide high-performance data management and retrieval, critical for the speed and efficiency of AI and ML operations. - Customizable Indexing: Vector databases offer customizable indexing options, enabling optimized data organization and retrieval based on specific needs. Vector databases and use cases. This section provides varies vector databases and their use case details. Faiss and ScaNN They are libraries that serve as crucial tools in the realm of vector search. These libraries provide functionality that is instrumental in managing and searching through vector data, making them invaluable resources in this specialized area of data management. Elasticsearch It’s a widely used search and analytics engine, has recently incorporated vector search capabilities. This new feature enhances its functionality, enabling it to handle and search through vector data more effectively. Pinecone It is a robust vector database with a unique set of features. It supports both dense and sparse vectors in its indexing functionality, which enhances its flexibility and adaptability."", ""metadata"": {""file_name"": ""Vector_Database.html"", ""doc_id"": ""abdaba3d5a02bafe"", ""source_type"": ""html"", ""chunk_id"": 1}, ""score"": 0.18606412410736084}, {""content"": ""It offers both fully managed and self-hosted solutions, catering to a variety of deployment preferences. One of Weaviate’s key features is its ability to store both vectors and objects, enhancing its data handling capabilities. It is widely used for a range of applications, including semantic search and data classification in ERP systems. In the e-commerce sector, it powers search and recommendation engines. Weaviate is also used for image search, anomaly detection, automated data harmonization, and cybersecurity threat analysis, showcasing its versatility across multiple domains. Redis Redis is a high-performing vector database known for its fast in-memory storage, offering low latency for read-write operations. This makes it an excellent choice for recommendation systems, search engines, and data analytics applications that require quick data access. Redis supports various data structures for vectors, including lists, sets, and sorted sets. It also provides vector operations such as calculating distances between vectors or finding intersections and unions. These features are particularly useful for similarity search, clustering, and content-based recommendation systems. In terms of scalability and availability, Redis excels in handling high throughput workloads and offers data replication. It also integrates well with other data types, including traditional relational databases (RDBMS). Redis includes a Publish/Subscribe (Pub/Sub) feature for real-time updates, which is beneficial for managing real-time vectors."", ""metadata"": {""source_type"": ""html"", ""file_name"": ""Vector_Database.html"", ""chunk_id"": 3, ""doc_id"": ""abdaba3d5a02bafe""}, ""score"": 0.1079704761505127}, {""content"": ""One of its key strengths lies in its ability to combine traditional search methods with AI-based dense vector search, creating a hybrid search approach that leverages the best of both worlds. Primarily cloud-based, Pinecone is designed for machine learning applications and integrates well with a variety of platforms, including GCP, AWS, Open AI, GPT-3, GPT-3.5, GPT-4, Catgut Plus, Elasticsearch, Haystack, and more. It’s important to note that Pinecone is a closed-source platform and is available as a Software as a Service (SaaS) offering. Given its advanced capabilities, Pinecone is particularly well-suited for the cybersecurity industry, where its high-dimensional search and hybrid search capabilities can be leveraged effectively to detect and respond to threats. Chroma It’s a vector database that has a Core-API with four primary functions, one of which includes an in-memory document-vector store. It also utilizes the Face Transformers library to vectorize documents, enhancing its functionality and versatility. Chroma is designed to operate both in the cloud and on-premises, offering flexibility based on user needs. Particularly, it excels in audio-related applications, making it an excellent choice for audio-based search engines, music recommendation systems, and other audio-related use cases. Weaviate It’s a versatile vector database that allows users to vectorize their content using either its built-in modules or custom modules, providing flexibility based on specific needs."", ""metadata"": {""chunk_id"": 2, ""doc_id"": ""abdaba3d5a02bafe"", ""file_name"": ""Vector_Database.html"", ""source_type"": ""html""}, ""score"": 0.08368897438049316}, {""content"": ""Moreover, Redis is lightweight and simple to use, making it a user-friendly solution for managing vector data. Milvus It’s a versatile vector database that offers an API like a document store, much like MongoDB. It stands out due to its support for a wide variety of data types, making it a popular choice in the data science and machine learning fields. One of Milvus’ unique features is its multi-vectorization capability, which allows users to specify at runtime the type of vector to use for the search. Furthermore, it utilizes Knowwhere, a library that sits atop other libraries like Faiss, to manage communication between queries and the vector search algorithms. Milvus also offers seamless integration with machine learning workflows, thanks to its compatibility with PyTorch and TensorFlow. This makes it an excellent tool for a range of applications, including e-commerce, image and video analysis, object recognition, image similarity search, and content-based image retrieval. In the realm of natural language processing, Milvus is used for document clustering, semantic search, and question-answering systems. For this solution, we picked milvus for the solution validation. For performance, we used both milvus and postgres(pgvecto.rs). Why we chose milvus for this solution? - Open-Source: Milvus is an open-source vector database, encouraging community-driven development and improvements."", ""metadata"": {""source_type"": ""html"", ""file_name"": ""Vector_Database.html"", ""chunk_id"": 4, ""doc_id"": ""abdaba3d5a02bafe""}, ""score"": 0.04169011116027832}, {""content"": ""Why Use PGVector? We’ve all been in this position before: trying to quickly solve a technical problem just to end up spending hours browsing through endless documentation in the hope of finding a solution. With all that time going to waste, there has to be a better way to search documentation. And now, there is. Instaclustr has recently introduced the PGVector extension to our PostgreSQL Managed Service, enabling users to quickly and easily search text data without the need for a specialized database to store your embeddings—a major development in the realm of textual data. Embeddings are a mathematical representation of how related or similar words are. For example, the words “tree” and “bush” would be more closely related than say, “tree” and “automobile.” Embeddings are generally stored as vectors, which are essentially a list of numbers. PGVector is an extension that provides the ability to search for vectors that are more closely related to each other. With embeddings and vector search, you can then search for text that semantically means the same thing—an incredibly useful tool, especially when building Large Language Models (LLMs). When PGVector comes into play, PostgreSQL turns into a highly performant vector store capable of performing distance-based search operations on embeddings."", ""metadata"": {""doc_id"": ""5c2b2c2734bfb5c2"", ""chunk_id"": 0, ""file_name"": ""How_To_Improve_Your_LLM_Accuracy_and_Performance_With_PGVector_and_PostgreSQL.html"", ""source_type"": ""html""}, ""score"": -0.04040825366973877}, {""content"": ""* Storage: it’s responsible for data persistence. It comprises meta storage, log broker, and object storage. NetApp storage such as ONTAP and StorageGRID provides object storage and File based storage to Milvus for both customer data and vector database data."", ""metadata"": {""chunk_id"": 6, ""file_name"": ""Vector_Database.html"", ""doc_id"": ""abdaba3d5a02bafe"", ""source_type"": ""html""}, ""score"": -0.049686312675476074}, {""content"": ""- AI Integration: It leverages embedding similarity search and AI applications to enhance vector database functionality. - Large Volume Handling: Milvus has the capacity to store, index, and manage over a billion embedding vectors generated by Deep Neural Networks (DNN) and Machine Learning (ML) models. - User-Friendly: It is easy to use, with setup taking less than a minute. Milvus also offers SDKs for different programming languages. - Speed: It offers blazing fast retrieval speeds, up to 10 times faster than some alternatives. - Scalability and Availability: Milvus is highly scalable, with options to scale up and out as needed. - Feature-Rich: It supports different data types, attribute filtering, User-Defined Function (UDF) support, configurable consistency levels, and travel time, making it a versatile tool for various applications. Milvus architecture overview This section provides higher lever components and services are used in Milvus architecture. * Access layer – It’s composed of a group of stateless proxies and serves as the front layer of the system and endpoint to users. * Coordinator service – it assigns the tasks to the worker nodes and act as a system's brain. It has three coordinator types: root coord,data coord and query coord. * Worker nodes : It follows the instruction from coordinator service and execute user triggered DML/DDL commands.it has three types of worker nodes such as query node, data node and index node."", ""metadata"": {""doc_id"": ""abdaba3d5a02bafe"", ""chunk_id"": 5, ""file_name"": ""Vector_Database.html"", ""source_type"": ""html""}, ""score"": -0.06490051746368408}]"
